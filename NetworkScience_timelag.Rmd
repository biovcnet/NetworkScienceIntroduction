---
title: "R Notebook"
output: html_notebook
---

The point of this lesson is to show that you can do a time lagged network in R, no LSA program required. This skips the "local" element of Local Similarity Analysis, but I found that in my research I was electing just to do time lagged spearman networks. Similarly, one could use this same approach to build a time lagged sparCC network or most other statistics.

This may or may not work for graphical lasso approaches.


```{r}
library(tidyverse)
library(lubridate)
library(zoo)
library(SpiecEasi)
library(psych)
library(igraph)
```
# Read in

Lets bring in some time-series data from the SPOT dataset. These are ARISA fragements, their lengths associate with species identity. I downloaded the data here:

https://www.bco-dmo.org/dataset/535915

And then removed some variables that we are not using yet. 

The data set is somewhat large. To save space, I've zipped it.

Fortunately, R can read zipped csv files just fine.
Some of the months are missing arrisa data and are loaded in as "nd". We can remove those later, but we tell R so it doesn't freak out.

```{r}
# read in data
spotSurface <- read_csv("SPOT/spot_arisa_surface.zip") %>%
  # treat date column as a date, rather than a character string
  mutate(date_local = ymd(date_local))
```

# Removing low occurance taxa

How many times do we see each taxon?
How abundant are they. 
We filter the taxa a lot if we only keep things with a mean abundance of at least 0.5%
```{r}
howMany <- spotSurface %>%
  group_by(arisa_frag) %>%
  summarize(sightings = sum(na.omit(rel_abund > 0)),
            meanAbun = mean(na.omit(rel_abund))) %>%
  arrange(-sightings)
 

keepTaxa <- howMany %>% 
  filter(sightings >= 5, meanAbun >= 0.01) %>%
  pull(arisa_frag)

keepTaxa
```

```{r}
spotSurface2 <- spotSurface %>% filter(arisa_frag %in% keepTaxa)
```


# Processing

Lets reshape everything into a wide format data frame

```{r}
spotWide <- spotSurface2 %>% pivot_wider(names_from = arisa_frag, values_from = rel_abund)
```

There are lots of months with missing data, and lots of months that are missing but don't have rows. Lets update this so that we have a row for every month, even if it has NA values.

```{r}
spotWide2 <- spotWide %>%
  na.omit %>%
  mutate(yr = year(date_local), mth = month(date_local)) %>%
  select(yr, mth, date_local, everything()) %>%
  arrange(yr, mth) 
```

```{r}

goalDates <- tibble(
  yr = rep(first(spotWide2$yr):last(spotWide2$yr), each = 12),
  mth = rep(1:12, last(spotWide2$yr) - first(spotWide2$yr) + 1)
) %>%
  mutate(filler_date = ymd(paste(yr, mth, 15, sep = "_"))) %>%
  filter(filler_date > min(spotWide2$date_local) &
           filler_date < max(spotWide2$date_local)) %>%
  select(-filler_date)
```

```{r}
spotWideAllMonths <- left_join(goalDates, spotWide2)
```

Ok. So we're going to address this missing not at random data in a way that critics might call reckless. Linearly  interpolating it.
```{r}
spotInterp <- spotWideAllMonths %>%
  mutate_at(vars(matches("ARISA")), na.approx)
```

```{r}
spotInterp2 <- spotInterp %>%
  mutate(date_local = if_else(
    is.na(date_local),
    ymd(paste(yr, mth, 15)),
    date_local)
  ) %>%
  select(-c(yr, mth))
```

```{r}
spotInterpMtx <- spotInterp2 %>%
  column_to_rownames("date_local") %>%
  as.matrix()

spotClrMtx <- clr(spotInterpMtx)
```


# Lags
Ok, so with this dataset, we can do any of the things from the earlier lessons.
We can also export it for local similarity analysis outside of R.

Ok, lets make a new matrix, where the columns are dates, but then there are another series of columns of dates lagged by one.

```{r}

spotUnLag <- spotInterpMtx[-1,]
spotLag <- spotInterpMtx[-nrow(spotInterpMtx),]
colnames(spotUnLag) <- paste("nolag", colnames(spotUnLag), sep = "-")
colnames(spotLag) <- paste("lag1", colnames(spotLag), sep = "-")
spotWLag <- cbind(spotUnLag, spotLag)
```

Now we'll correlate everything vs everyting. Keep in mind that the unlagged vs unlagged are missing one row. We could get around this by doing everything in two batches.

Lets skip Sparcc for now
```{r}
# tp0 <- proc.time()
# lagSparcc <- sparcc(spotWLag)
# tp1 <- proc.time()
# tp1 - tp0
```

# Calculating clr-spearman matrix

## Non lagged data

```{r}
spearCorTestClr <- corr.test(spotInterpMtx, method = "spearman", adjust = "none")
spearCorClr <- spearCorTestClr$r
spearPClr <- spearCorTestClr$p
```


## Lagged data

```{r}
spearCorTestClr_Lagged <- corr.test(spotWLag, method = "spearman", adjust = "none")
spearCorClr_Lagged <- spearCorTestClr_Lagged$r
spearPClr_Lagged <- spearCorTestClr_Lagged$p
```

```{r}
source("jacob_library.R")
```

Some data wrangling, as per my earlier lesson

```{r}
reordered_spearCorTestClr_Lagged <- reorder_cor_and_p(spearCorClr_Lagged, spearPClr_Lagged)
spearCorClr_Lagged_Reordered <- reordered_spearCorTestClr_Lagged$r
spearPClr_Lagged_Reordered <- reordered_spearCorTestClr_Lagged$p
```

```{r}
spearCorClr_Lagged_Proc <- spearCorClr_Lagged_Reordered %>% get_upper_tri() %>% reshape2::melt() %>% na.omit() %>% rename(rho = value)
spearPClr_Lagged_Proc <- spearPClr_Lagged_Reordered %>% get_upper_tri() %>% reshape2::melt() %>% na.omit() %>% rename(p = value)
```

```{r}
spearRhoP_lagged <- left_join(spearCorClr_Lagged_Proc, spearPClr_Lagged_Proc, by = c("Var1", "Var2"))
spearRhoP_lagged
```

## Parsing out the lags



```{r}
spearRhoP_lagged2 <- spearRhoP_lagged %>% 
  separate(Var1, c("V1", "Var1"), sep = "-") %>%
  separate(Var2, c("V2", "Var2"), sep = "-") %>%
  # get rid of rows where both variables are laged
  filter(!(V1 == "lag1" & V2 == "lag1")) %>%
  filter(Var1 != Var2) %>%
  mutate(delay = if_else(V1 == "lag1" & V2 == "nolag", -1,
         if_else(V1 == "nolag" & V2 == "lag1", 1,
                 if_else(V1 == "nolag" & V2 == "nolag", 0, -9999)
         )
  )
  ) %>%
  mutate(fdr =p.adjust(p, method = "BH"))
  
spearRhoP_lagged2 
```
Now we select the delay with the highest score.

Note that we calculated the false discovery rate *before* we selected the value with the highest score, but *after* we removed the delay-vs-delay comparasons.

```{r}
spearRhoP_lagged3 <- spearRhoP_lagged2 %>%
  group_by(Var1, Var2) %>% 
  top_n(1, rho) %>%
  select(-c(V1, V2))
spearRhoP_lagged3
```

And there you have a table that can go into a network.
Some thoughts. There is way more data handling to do this with sparcc, but it could be done.
I really ought to automate this into a general time delay function.
I don't think this works for graphical lasoo but it could be tried.

Autocorrelation may inflate some of these scores.

# Plotting

I want a network where arrows point from leading to lagging nodes. Unlagged connections should be represented as lines.

```{r}
spearRhoP_lagged4 <- spearRhoP_lagged3 %>%
  filter(fdr < 0.05) %>%
  mutate(arrow = recode(delay, `-1` = "<", `1` = ">", `0` = "-"))
```


```{r}
LaggedSpearGraph <- graph_from_data_frame(spearRhoP_lagged4)
```

```{r}
LaggedSpearGraph
```

```{r}
set.seed(333)
plot(LaggedSpearGraph,vertex.size=2, vertex.label.cex = 0.75, edge.arrow.mode = E(LaggedSpearGraph)$arrow, vertex.label = NA, edge.arrow.size = .5)
```

Clearly, I'm not an igraph artist but you get the idea.


