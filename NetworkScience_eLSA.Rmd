---
title: "NetworkScience Extended Local Similarity Analysis (eLSA)"
author: "Jacob Cram"
date: "5/27/2020"
output: html_notebook
---
# Getting Ready
Every time I ever run elsa, I look at the help file.

```{r}
library(tidyverse) # Because of course
```


```{bash engine.opts='-i'}
conda activate elsa01
lsa_compute -h
```


```{r}
arisa <- read.delim("SPOT/spotClrMtx.tsv", sep = "\t")
dim(arisa)
```

# Example Run
One thing you need to know is the number of columns, its 119. Here's an example of working code, lets run it and look at what it does. 

See here for a fuller description of inputs and outputs https://bitbucket.org/charade/elsa/wiki/Manual

```{bash engine.opts='-i'}
conda activate elsa01
lsa_compute SPOT/spotClrMtx.tsv SPOT/spotClrMtx.lsa.tsv -d 1 -m 0 -p theo -x 1000 -b 0 -r 1 -s 119 -n none
```

`dataFile` is our input: `SPOT/spotClrMtx.tsv`
`resultFile` is our output: `SPOT/spotClrMtx.out.tsv`

I put these in as the first two arguments

`-d` The *delay* that is how much do I want to allow things to lag. For a monthly data set, I use one. That is, I am intrested in cases where one variable changes, and then another responds one month later.

`-m` How many times does a species have to show up in the data set for us to not have to throw it out. We've already done filtering in `R` so I'm going to tell it not to toss anything by setting this to `0`

`-p` This is the method that it uses to compute the *p*-value for just the *LS* score. So if you are interested in "global" associations, eg spearman or pearson and not *LS*, you want the fastest one, which is theo. If you do care about *LS* score, you should run with theo first to make sure things work. This means that the p-value is estemated with an equation, rather than actual permutation testing. Since theo can be wrong, you probably don't want to bet the farm on it. `perm` is slowist and permutes everyting `-x` times. `mix` does *theo* unless the permutation falls near some dectection threshold in which case it runs *perm*

`-x` How many permutations to use if you specify `-p theo` or `-p mix`.

`-b` This is relevant if you have replicates. We don't so I set it to `0`. 

`-r` The number of replicates you have. We don't have replicates so I set it to `1`

`-s` The number of samples (timepoints) that you have. `119` in our case. See above.

`-n` How do you want to normalize the data. We already did and then we `clr` transformed it and we don't want to mess with that so I set it to none.

And thats it.

# Output file.
Lets look at the output file and talk about what each thing means.

```{r message = FALSE}
lsaOutput <- read_tsv("SPOT/spotClrMtx.lsa.tsv")
head(lsaOutput)
```


First, each row gives statistical information about a pair of variables, each column is a different summary variable.

`X` The first of two variables.
`Y` The second of two variables
`LS` The *local similarity* score of those two variables. Read this paper to understand what exactly an LS score is. Suffice it to say, it looks for patterns that may exist over only part of the dataset, in additions to ones that exist everywhere, especially if those patterns are strong enough to compensate for them only being over part of the data set.
`lowCI` and `upCI` If we had replicates, there would be confedence intervals around the `LS` score. We don't so these equal `LS`. Don't take them to mean anything.
`Xs` Since there is a local association that may occur over part of the dataset this is where the association starts.
`Ys` Where it starts for the *Y* variable. Usually offset from `Xs` by the delay.
`Len` The length of the region where the local similarity occurs. Ones that approach 119 - Delay exist over more of the dataset, smaller ones exist over less of the dataset.
`Delay` How much of a delay there is. If zero, we are looking at unlagged asoicaitons. If `1` then `X` changes first, then `Y`;if `-1` then `Y` changes, then `X`. I might have this backward so check me on it and let me know. Apparently, `Delay` is calculated from `Xs` and `Ys` rather than visa versa.
`P` the P value for the `LS` score, generated by whatever method you specified in `-p`

Now we get into other statistics besides `LS`. Back in the day, LSA only reported `LS`, but I wanted time lagged statistics that covered the whole, rather than part of the data, and didn't know how to code these up in R yet. So we have pearson and spearman lagged and unlagged associations.

`PCC` Pearson correlation coefficient, **unlagged**.
`Ppcc` *p*-value for `PCC`
`SPCC` Pearson correlation coefficient, **lagged**.
`Pspcc` *p*-value for `SPCC`
`Dspcc` The delay for the lagged pearson

`SCC` Spearman correlation coefficient, **unlagged**.
`Pscc` *p*-value for `SCC`
`SSCC` Spearman correlation coefficient, **lagged** .
`Psscc` *p*-value for `SSCC`
`Dsscc` The delay for the lagged spearman

Then we have q values. Q values are a way of adjusting for the false discoveries we would have with our many *p*-values. Read stuff by Storey, like 

Dabney A, Storey JD. Q-value estimation for false discovery rate control. Medicine. 2004;344:539â€“548. 

to lean more.

`Q` q-value for `LS
`Qpcc` q-value for `PCC`
`Qspcc` q-value for `SPCC`
`Qscc` q-value for `SCC`
`Qsscc` q-value for `SSCC`

I have no idea what `Xi` and `Yi` do and I can't find that info in the docs either. They look like a recoding of the `X` and `Y` variables as unique integers, which is maybe a thing that gets used internally. I advise ignoring them unless you have some reason not to.

Generally I use `SSCC`, `Psscc` and `Qsscc` and `Dsscc` and not much else.

I use LS before *Liquid Analysis*, which we'll get into in another lesson, in which case I care about
`LS`, `Xs`, `Ys`, `Delay`, `P` and `Q`.






